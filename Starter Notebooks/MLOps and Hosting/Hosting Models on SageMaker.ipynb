{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hosting Models on SageMaker and Automate the Workflow\n",
    "\n",
    "In this module you will:\n",
    "- Host a pretrained SKLearn model on SageMaker\n",
    "- Enable autoscaling on your endpoint \n",
    "- Monitor your model\n",
    "- Perform hyperparameter tuning\n",
    "- Redploy a new model to the endpoint\n",
    "- Automate the pipeline using the notebook runner toolkit\n",
    "\n",
    "Let's get started! \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Access your model artifact\n",
    "First, you should see a `model.tar.gz` file in this repository. Let's get that in your S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker will check to make sure this is a valid tar.gz object\n",
    "local_model_file = 'model.tar.gz'\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'model-hosting'\n",
    "\n",
    "s3_path = 's3://{}/{}/'.format(bucket, prefix)\n",
    "\n",
    "msg = 'aws s3 cp {} {}'.format(local_model_file, s3_path)\n",
    "\n",
    "os.system(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load your pretrained model artifact into SageMaker\n",
    "Now, we know that this model was trained using the SKLearn container within SageMaker. All we need to do get this into a SageMaker-managed endpoint is set it up as a model. Let's do that here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-181880743555/model-hosting/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_data = '{}{}'.format(s3_path, local_model_file)\n",
    "print (model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    regr = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return regr\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    '''return the class and the probability of the class'''\n",
    "    prediction = model.predict(input_data)\n",
    "    pred_prob = model.predict_proba(input_data) #a numpy array\n",
    "    return np.array(pred_prob)\n",
    "\n",
    "def parse_args():\n",
    "    \n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--max_leaf_nodes', type=int, default=-1)\n",
    "\n",
    "   # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default = os.environ['SM_CHANNEL_TEST'])\n",
    "    \n",
    "    # hyperparameters for tuning\n",
    "    parser.add_argument('--batch-size', type=int, default=256)\n",
    "    parser.add_argument('--lr', type=float, default = 0.001)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args\n",
    "\n",
    "def train(args):\n",
    "    \n",
    "   # Take the set of files and read them all into a single pandas dataframe\n",
    "    train_data=pd.read_csv(os.path.join(args.train, 'train_set.csv'), engine='python')\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data['truth']\n",
    "    train_X = train_data[train_data.columns[1:len(train_data)]]\n",
    "\n",
    "    # Now use scikit-learn's MLP Classifier to train the model.\n",
    "\n",
    "    regr = MLPClassifier(random_state=1, max_iter=500, batch_size = args.batch_size, learning_rate_init = args.lr, solver='lbfgs').fit(train_X, train_y)\n",
    "    regr.get_params()\n",
    "\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(regr, os.path.join(args.model_dir, \"model.joblib\")) \n",
    "    \n",
    "    return regr\n",
    "    \n",
    "def accuracy(y_pred, y_true):\n",
    "    \n",
    "    cm = confusion_matrix(y_pred, y_true)\n",
    "    \n",
    "    diagonal_sum = cm.trace()\n",
    "    sum_of_all_elements = cm.sum()\n",
    "    \n",
    "    rt = diagonal_sum / sum_of_all_elements\n",
    "    \n",
    "    print ('Accuracy: {}'.format(rt))\n",
    "    \n",
    "    return rt\n",
    "    \n",
    "    \n",
    "def test(regr, args):\n",
    "    test_data=pd.read_csv(os.path.join(args.test, 'test_set.csv'), engine='python')\n",
    "\n",
    "    # labels are in the first column\n",
    "    y_true = test_data['truth']\n",
    "    test_x = test_data[test_data.columns[1:len(test_data)]]\n",
    "    \n",
    "    y_pred = regr.predict(test_x)\n",
    "    \n",
    "    accuracy(y_pred, y_true)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "    \n",
    "    regr = train(args)\n",
    "    \n",
    "    test(regr, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "model = SKLearnModel(model_data = model_data,\n",
    "                     role = role, \n",
    "                     framework_version = '0.20.0', \n",
    "                     py_version='py3',\n",
    "                     entry_point = 'train.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create an Endpoint on SageMaker\n",
    "Now, here comes the complex maneuver. Kidding, it's dirt simple. Let's turn your model into a RESTful API!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "predictor = model.deploy(1, 'ml.m4.2xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker.sklearn.model import SKLearnPredictor\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# optional. If your kernel times out, or your need to refresh, here's how you can easily point to an existing endpoint\n",
    "endpoint_name = 'sagemaker-scikit-learn-2020-10-14-15-12-50-644'\n",
    "\n",
    "predictor = SKLearnPredictor(endpoint_name = endpoint_name, sagemaker_session = sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's get some predictions from that endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_set['truth']\n",
    "\n",
    "test_set.drop('truth', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "y_pred = pd.DataFrame(predictor.predict(test_set))\n",
    "\n",
    "assert len(y_pred) == test_set.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Enable Autoscaling on your Endpoint\n",
    "For the sake of argument, let's say we're happy with this model and want to continue supporting it in prod. Our next step might be to enable autoscaling. Let's do that right here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def get_resource_id(endpoint_name):\n",
    "\n",
    "    client = boto3.client('sagemaker')\n",
    "\n",
    "    response = client.describe_endpoint(\n",
    "        EndpointName=endpoint_name)\n",
    "\n",
    "    variant_name = response['ProductionVariants'][0]['VariantName']\n",
    "    resource_id = 'endpoint/{}/variant/{}'.format(endpoint_name, variant_name)\n",
    "    \n",
    "    return resource_id\n",
    "\n",
    "resource_id = get_resource_id(endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "def set_scaling_policy(resource_id, min_capacity = 1, max_capacity = 8, role = role):\n",
    "\n",
    "    scaling_client = boto3.client('application-autoscaling')\n",
    "\n",
    "    response = scaling_client.register_scalable_target(\n",
    "        ServiceNamespace='sagemaker',\n",
    "        ResourceId=resource_id,\n",
    "        ScalableDimension='sagemaker:variant:DesiredInstanceCount',\n",
    "        MinCapacity=min_capacity,\n",
    "        MaxCapacity=max_capacity,\n",
    "        RoleARN=role)\n",
    "    \n",
    "    return response\n",
    "\n",
    "res = set_scaling_policy(resource_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Enable Model Monitor on your Endpoint\n",
    "Now that you have a model up and running, with autoscaling enabled, let's set up model monitor on that endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "about to set up monitoring for endpoint named sagemaker-scikit-learn-2020-10-14-15-12-50-644\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import os\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "prefix = 'model-hosting'\n",
    "\n",
    "s3_capture_upload_path =  's3://{}/{}/model-monitor'.format(bucket, prefix)\n",
    "\n",
    "print ('about to set up monitoring for endpoint named {}'.format(endpoint_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's set up a data capture config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'sagemaker-scikit-learn-2020-10-14-15-12-50-644',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:181880743555:endpoint/sagemaker-scikit-learn-2020-10-14-15-12-50-644',\n",
       " 'EndpointConfigName': 'sagemaker-scikit-learn-2020-10-14-15-12-2020-10-14-15-23-50-458',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:0.20.0-cpu-py3',\n",
       "     'ResolvedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn@sha256:1476311657fe444be8f95f472a8a91a3da577d558e1046678ee76c611f7132ca',\n",
       "     'ResolutionTime': datetime.datetime(2020, 10, 14, 15, 23, 53, 899000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'DataCaptureConfig': {'EnableCapture': True,\n",
       "  'CaptureStatus': 'Started',\n",
       "  'CurrentSamplingPercentage': 50,\n",
       "  'DestinationS3Uri': 's3://sagemaker-us-east-1-181880743555/model-hosting/model-monitor'},\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2020, 10, 14, 15, 12, 50, 973000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2020, 10, 14, 15, 31, 4, 293000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '851543a5-fc30-46cd-99a5-9344bf589ee8',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '851543a5-fc30-46cd-99a5-9344bf589ee8',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1006',\n",
       "   'date': 'Wed, 14 Oct 2020 15:31:24 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "\n",
    "data_capture_config = DataCaptureConfig(\n",
    "                        enable_capture = True,\n",
    "                        sampling_percentage=50,\n",
    "                        destination_s3_uri=s3_capture_upload_path,\n",
    "                        capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "                        csv_content_types=[\"text/csv\"],\n",
    "                        json_content_types=[\"application/json\"])\n",
    "\n",
    "# Now it is time to apply the new configuration and wait for it to be applied\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "\n",
    "sess.wait_for_endpoint(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step here is to pass in our training data, and ask SageMaker to learn baseline thresholds for all of our features. \n",
    "\n",
    "First, let's make sure the data we used to train our model is stored in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = 'aws s3 cp train_set.csv s3://{}/{}/train/'.format(bucket, prefix)\n",
    "os.system(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - show them how to get access to this training data\n",
    "s3_training_data_path = 's3://{}/{}/train/train_set.csv'.format(bucket, prefix)\n",
    "\n",
    "s3_baseline_results =  's3://{}/{}/model-monitor/baseline-results'.format(bucket, prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2020-10-14-15-46-58-310\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-181880743555/model-hosting/train/train_set.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-181880743555/model-hosting/model-monitor/baseline-results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "........................\u001b[34m2020-10-14 15:50:44,484 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:181880743555:processing-job/baseline-suggestion-job-2020-10-14-15-46-58-310', 'ProcessingJobName': 'baseline-suggestion-job-2020-10-14-15-46-58-310', 'Environment': {'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-181880743555/model-hosting/train/train_set.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-181880743555/model-hosting/model-monitor/baseline-results', 'S3UploadMode': 'EndOfJob'}}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'RoleArn': 'arn:aws:iam::181880743555:role/service-role/AmazonSageMaker-ExecutionRole-20200929T125134', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,485 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,485 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": false, \"output_columns_position\": \"START\"}}, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"output_path\": \"/opt/ml/processing/output\", \"start_time\": null, \"end_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\"}\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,485 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,485 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,538 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,539 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,539 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'hosts': ['algo-1']}\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,547 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,547 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,547 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,981 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.2.216.41\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yar\u001b[0m\n",
      "\u001b[34mn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_265\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,991 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:44,994 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-03fbbccd-84ba-4b9c-90b5-ef591644cec2\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,429 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,439 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,440 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,442 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,446 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,446 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,446 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,447 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,475 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,486 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,486 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,492 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,493 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Oct 14 15:50:45\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,494 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,494 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,495 INFO util.GSet: 2.0% max memory 3.1 GB = 63.5 MB\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,495 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,528 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,531 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,532 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,532 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,532 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,532 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,561 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,561 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,561 INFO util.GSet: 1.0% max memory 3.1 GB = 31.8 MB\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,561 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,563 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,563 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,563 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,563 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,567 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,570 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,570 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,570 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,570 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,607 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,608 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,608 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,611 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,611 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,613 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,613 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,613 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 976.0 KB\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,613 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,633 INFO namenode.FSImage: Allocated new BlockPoolId: BP-117402642-10.2.216.41-1602690645628\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,644 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,650 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,720 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,731 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,734 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.2.216.41\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:45,744 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:47,787 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:47,787 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:49,847 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:49,847 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:51,929 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:51,929 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:54,025 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:54,025 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:56,096 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:50:56,096 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:06,104 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 WARN  NativeCodeLoader:60 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  Main:27 - Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  Main:30 - Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  FileUtil:66 - Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SparkContext:54 - Running Spark version 2.3.1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SparkContext:54 - Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  Utils:54 - Successfully started service 'sparkDriver' on port 41587.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SparkEnv:54 - Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  SparkEnv:54 - Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  BlockManagerMasterEndpoint:54 - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  BlockManagerMasterEndpoint:54 - BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  DiskBlockManager:54 - Created local directory at /tmp/blockmgr-e0f989b7-9b4a-41f7-99e5-3f14296ed459\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:07 INFO  MemoryStore:54 - MemoryStore started with capacity 1458.6 MB\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:08 INFO  SparkEnv:54 - Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:08 INFO  SparkContext:54 - Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.2.216.41:41587/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1602690668052\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:08 INFO  RMProxy:133 - Connecting to ResourceManager at /10.2.216.41:8032\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:08 INFO  Client:54 - Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  Configuration:2636 - resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  ResourceUtils:427 - Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  Client:54 - Verifying our application has not requested more than the maximum memory capability of the cluster (15578 MB per container)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  Client:54 - Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  Client:54 - Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  Client:54 - Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:09 INFO  Client:54 - Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:10 WARN  Client:66 - Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:11 INFO  Client:54 - Uploading resource file:/tmp/spark-a28f1950-201e-4164-99d3-a146f86f382a/__spark_libs__6034671535784262710.zip -> hdfs://10.2.216.41/user/root/.sparkStaging/application_1602690651138_0001/__spark_libs__6034671535784262710.zip\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  Client:54 - Uploading resource file:/tmp/spark-a28f1950-201e-4164-99d3-a146f86f382a/__spark_conf__1519363227791618873.zip -> hdfs://10.2.216.41/user/root/.sparkStaging/application_1602690651138_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  SecurityManager:54 - Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  SecurityManager:54 - Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  SecurityManager:54 - Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  SecurityManager:54 - Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  Client:54 - Submitting application application_1602690651138_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  YarnClientImpl:310 - Submitted application application_1602690651138_0001\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:12 INFO  SchedulerExtensionServices:54 - Starting Yarn extension services with app application_1602690651138_0001 and attemptId None\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:13 INFO  Client:54 - Application report for application_1602690651138_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:13 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1602690672727\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1602690651138_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:14 INFO  Client:54 - Application report for application_1602690651138_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:15 INFO  Client:54 - Application report for application_1602690651138_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:16 INFO  Client:54 - Application report for application_1602690651138_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  YarnClientSchedulerBackend:54 - Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1602690651138_0001), /proxy/application_1602690651138_0001\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  YarnSchedulerBackend$YarnSchedulerEndpoint:54 - ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  Client:54 - Application report for application_1602690651138_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  Client:54 - \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.2.216.41\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: 0\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1602690672727\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1602690651138_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  YarnClientSchedulerBackend:54 - Application application_1602690651138_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43771.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  NettyBlockTransferService:54 - Server created on 10.2.216.41:43771\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(driver, 10.2.216.41, 43771, None)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  BlockManagerMasterEndpoint:54 - Registering block manager 10.2.216.41:43771 with 1458.6 MB RAM, BlockManagerId(driver, 10.2.216.41, 43771, None)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(driver, 10.2.216.41, 43771, None)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(driver, 10.2.216.41, 43771, None)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:17 INFO  log:192 - Logging initialized @11697ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:20 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.2.216.41:37870) with ID 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:20 INFO  BlockManagerMasterEndpoint:54 - Registering block manager algo-1:38349 with 5.8 GB RAM, BlockManagerId(1, algo-1, 38349, None)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:38 INFO  YarnClientSchedulerBackend:54 - SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:38 WARN  SparkContext:66 - Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:38 INFO  DatasetReader:90 - Files to process:List(file:///opt/ml/processing/input/baseline_dataset_input/train_set.csv)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:38 INFO  SharedState:54 - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.3.1/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:38 INFO  SharedState:54 - Warehouse path is 'file:/usr/spark-2.3.1/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:38 INFO  StateStoreCoordinatorRef:54 - Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  FileSourceStrategy:54 - Post-Scan Filters: (length(trim(value#0, None)) > 0)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  CodeGenerator:54 - Code generated in 163.231751 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  CodeGenerator:54 - Code generated in 23.797512 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:39 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 429.5 KB, free 1458.2 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on 10.2.216.41:43771 (size: 38.3 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  SparkContext:54 - Created broadcast 0 from csv at DatasetReader.scala:49\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  SparkContext:54 - Starting job: csv at DatasetReader.scala:49\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  DAGScheduler:54 - Got job 0 (csv at DatasetReader.scala:49) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  DAGScheduler:54 - Final stage: ResultStage 0 (csv at DatasetReader.scala:49)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  DAGScheduler:54 - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at DatasetReader.scala:49), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 9.4 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.5 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on 10.2.216.41:43771 (size: 4.5 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  SparkContext:54 - Created broadcast 1 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at DatasetReader.scala:49) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  YarnScheduler:54 - Adding task set 0.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:40 INFO  TaskSetManager:54 - Starting task 0.0 in stage 0.0 (TID 0, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8344 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  BlockManagerInfo:54 - Added broadcast_1_piece0 in memory on algo-1:38349 (size: 4.5 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  BlockManagerInfo:54 - Added broadcast_0_piece0 in memory on algo-1:38349 (size: 38.3 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  TaskSetManager:54 - Finished task 0.0 in stage 0.0 (TID 0) in 1649 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  YarnScheduler:54 - Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  DAGScheduler:54 - ResultStage 0 (csv at DatasetReader.scala:49) finished in 1.735 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  DAGScheduler:54 - Job 0 finished: csv at DatasetReader.scala:49, took 1.784835 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  FileSourceStrategy:54 - Output Data Schema: struct<value: string>\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:41 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  CodeGenerator:54 - Code generated in 8.002448 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 429.5 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.7 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  BlockManagerInfo:54 - Added broadcast_2_piece0 in memory on 10.2.216.41:43771 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  SparkContext:54 - Created broadcast 2 from csv at DatasetReader.scala:49\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  FileSourceStrategy:54 - Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  FileSourceStrategy:54 - Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  FileSourceStrategy:54 - Output Data Schema: struct<_c0: string, _c1: string, _c2: string, _c3: string, _c4: string ... 29 more fields>\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  FileSourceScanExec:54 - Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 429.5 KB, free 1457.3 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 38.3 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on 10.2.216.41:43771 (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  SparkContext:54 - Created broadcast 3 from cache at DataAnalyzer.scala:76\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  FileSourceScanExec:54 - Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 WARN  Utils:66 - Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  CodeGenerator:54 - Code generated in 52.346803 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  CodeGenerator:54 - Code generated in 36.613433 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  SparkContext:54 - Starting job: head at DataAnalyzer.scala:79\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Got job 1 (head at DataAnalyzer.scala:79) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Final stage: ResultStage 1 (head at DataAnalyzer.scala:79)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Missing parents: List()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Submitting ResultStage 1 (MapPartitionsRDD[17] at head at DataAnalyzer.scala:79), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 45.5 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 16.7 KB, free 1457.2 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on 10.2.216.41:43771 (size: 16.7 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  SparkContext:54 - Created broadcast 4 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[17] at head at DataAnalyzer.scala:79) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  YarnScheduler:54 - Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  TaskSetManager:54 - Starting task 0.0 in stage 1.0 (TID 1, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8344 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  BlockManagerInfo:54 - Added broadcast_4_piece0 in memory on algo-1:38349 (size: 16.7 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  BlockManagerInfo:54 - Added broadcast_3_piece0 in memory on algo-1:38349 (size: 38.3 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  BlockManagerInfo:54 - Added rdd_11_0 in memory on algo-1:38349 (size: 142.4 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  TaskSetManager:54 - Finished task 0.0 in stage 1.0 (TID 1) in 567 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - ResultStage 1 (head at DataAnalyzer.scala:79) finished in 0.615 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  YarnScheduler:54 - Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:42 INFO  DAGScheduler:54 - Job 1 finished: head at DataAnalyzer.scala:79, took 0.622481 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  CodeGenerator:54 - Code generated in 45.543218 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  SparkContext:54 - Starting job: collect at AnalysisRunner.scala:313\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Registering RDD 22 (collect at AnalysisRunner.scala:313)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Got job 2 (collect at AnalysisRunner.scala:313) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Final stage: ResultStage 3 (collect at AnalysisRunner.scala:313)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 2 (MapPartitionsRDD[22] at collect at AnalysisRunner.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 299.5 KB, free 1456.9 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 94.2 KB, free 1456.8 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on 10.2.216.41:43771 (size: 94.2 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  SparkContext:54 - Created broadcast 5 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[22] at collect at AnalysisRunner.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  YarnScheduler:54 - Adding task set 2.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  TaskSetManager:54 - Starting task 0.0 in stage 2.0 (TID 2, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8333 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:43 INFO  BlockManagerInfo:54 - Added broadcast_5_piece0 in memory on algo-1:38349 (size: 94.2 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  TaskSetManager:54 - Finished task 0.0 in stage 2.0 (TID 2) in 1824 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  YarnScheduler:54 - Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - ShuffleMapStage 2 (collect at AnalysisRunner.scala:313) finished in 1.853 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - waiting: Set(ResultStage 3)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - Submitting ResultStage 3 (MapPartitionsRDD[25] at collect at AnalysisRunner.scala:313), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 364.6 KB, free 1456.4 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 106.6 KB, free 1456.3 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on 10.2.216.41:43771 (size: 106.6 KB, free: 1458.3 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  SparkContext:54 - Created broadcast 6 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[25] at collect at AnalysisRunner.scala:313) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  YarnScheduler:54 - Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  TaskSetManager:54 - Starting task 0.0 in stage 3.0 (TID 3, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  BlockManagerInfo:54 - Added broadcast_6_piece0 in memory on algo-1:38349 (size: 106.6 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:45 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 0 to 10.2.216.41:37870\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  TaskSetManager:54 - Finished task 0.0 in stage 3.0 (TID 3) in 604 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  YarnScheduler:54 - Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - ResultStage 3 (collect at AnalysisRunner.scala:313) finished in 0.630 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Job 2 finished: collect at AnalysisRunner.scala:313, took 2.508509 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 2\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 56\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 66\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned shuffle 0\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 16\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 28\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 6\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 31\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on algo-1:38349 in memory (size: 16.7 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_4_piece0 on 10.2.216.41:43771 in memory (size: 16.7 KB, free: 1458.3 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 24\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 51\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 26\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 14\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 50\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 11\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 47\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 19\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 12\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 10\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 52\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 21\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 59\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 44\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on algo-1:38349 in memory (size: 38.3 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_0_piece0 on 10.2.216.41:43771 in memory (size: 38.3 KB, free: 1458.3 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 0\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on 10.2.216.41:43771 in memory (size: 106.6 KB, free: 1458.4 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_6_piece0 on algo-1:38349 in memory (size: 106.6 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 32\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 4\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 20\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 42\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 46\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 62\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 25\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 22\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 9\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 48\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 23\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 18\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 45\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_2_piece0 on 10.2.216.41:43771 in memory (size: 38.3 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 65\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 17\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 8\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 55\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 57\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 3\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 5\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on 10.2.216.41:43771 in memory (size: 4.5 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_1_piece0 on algo-1:38349 in memory (size: 4.5 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 49\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 35\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 29\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 67\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 61\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 63\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 58\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 60\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 43\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 54\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 27\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 34\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 15\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 64\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 7\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on 10.2.216.41:43771 in memory (size: 94.2 KB, free: 1458.6 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Removed broadcast_5_piece0 on algo-1:38349 in memory (size: 94.2 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 33\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 13\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 53\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  ContextCleaner:54 - Cleaned accumulator 30\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  SparkContext:54 - Starting job: countByKey at ColumnProfiler.scala:566\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Registering RDD 32 (countByKey at ColumnProfiler.scala:566)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Got job 3 (countByKey at ColumnProfiler.scala:566) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Final stage: ResultStage 5 (countByKey at ColumnProfiler.scala:566)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 4 (MapPartitionsRDD[32] at countByKey at ColumnProfiler.scala:566), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 38.2 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 16.4 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on 10.2.216.41:43771 (size: 16.4 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  SparkContext:54 - Created broadcast 7 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[32] at countByKey at ColumnProfiler.scala:566) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  YarnScheduler:54 - Adding task set 4.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  TaskSetManager:54 - Starting task 0.0 in stage 4.0 (TID 4, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8333 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:46 INFO  BlockManagerInfo:54 - Added broadcast_7_piece0 in memory on algo-1:38349 (size: 16.4 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  TaskSetManager:54 - Finished task 0.0 in stage 4.0 (TID 4) in 915 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  YarnScheduler:54 - Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - ShuffleMapStage 4 (countByKey at ColumnProfiler.scala:566) finished in 0.935 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - waiting: Set(ResultStage 5)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Submitting ResultStage 5 (ShuffledRDD[33] at countByKey at ColumnProfiler.scala:566), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 3.2 KB, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 1919.0 B, free 1458.1 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on 10.2.216.41:43771 (size: 1919.0 B, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  SparkContext:54 - Created broadcast 8 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 5 (ShuffledRDD[33] at countByKey at ColumnProfiler.scala:566) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  YarnScheduler:54 - Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  TaskSetManager:54 - Starting task 0.0 in stage 5.0 (TID 5, algo-1, executor 1, partition 0, NODE_LOCAL, 7660 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  BlockManagerInfo:54 - Added broadcast_8_piece0 in memory on algo-1:38349 (size: 1919.0 B, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 1 to 10.2.216.41:37870\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  TaskSetManager:54 - Finished task 0.0 in stage 5.0 (TID 5) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  YarnScheduler:54 - Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - ResultStage 5 (countByKey at ColumnProfiler.scala:566) finished in 0.083 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Job 3 finished: countByKey at ColumnProfiler.scala:566, took 1.030625 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  ConstraintGenerator:45 - Generating Constraints:\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  ConstraintGenerator:50 - Constraints: {\n",
      "  \"version\" : 0.0,\n",
      "  \"features\" : [ {\n",
      "    \"name\" : \"_c0\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0,\n",
      "    \"string_constraints\" : {\n",
      "      \"domains\" : [ \"0\", \"truth\", \"1\" ]\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c1\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c2\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c3\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c4\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c5\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c6\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c7\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c8\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c9\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c10\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c11\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c12\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c13\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c14\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c15\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c16\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c17\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c18\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c19\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c20\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c21\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c22\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c23\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c24\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c25\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c26\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c27\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c28\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c29\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  }, {\n",
      "    \"name\" : \"_c30\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"completeness\" : 1.0\n",
      "  } ],\n",
      "  \"monitoring_config\" : {\n",
      "    \"evaluate_constraints\" : \"Enabled\",\n",
      "    \"emit_metrics\" : \"Enabled\",\n",
      "    \"datatype_check_threshold\" : 1.0,\n",
      "    \"domain_content_threshold\" : 1.0,\n",
      "    \"distribution_constraints\" : {\n",
      "      \"perform_comparison\" : \"Enabled\",\n",
      "      \"comparison_threshold\" : 0.1,\n",
      "      \"comparison_method\" : \"Robust\"\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  FileUtil:29 - Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  StatsGenerator:65 - Generating Stats:\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  CodeGenerator:54 - Code generated in 42.24184 ms\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  CodeGenerator:54 - Code generated in 10.726749 ms\u001b[0m\n",
      "\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  SparkContext:54 - Starting job: count at StatsGenerator.scala:67\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Registering RDD 38 (count at StatsGenerator.scala:67)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Got job 4 (count at StatsGenerator.scala:67) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Final stage: ResultStage 7 (count at StatsGenerator.scala:67)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Parents of final stage: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Missing parents: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Submitting ShuffleMapStage 6 (MapPartitionsRDD[38] at count at StatsGenerator.scala:67), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  MemoryStore:54 - Block broadcast_9 stored as values in memory (estimated size 37.3 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  MemoryStore:54 - Block broadcast_9_piece0 stored as bytes in memory (estimated size 16.0 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on 10.2.216.41:43771 (size: 16.0 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  SparkContext:54 - Created broadcast 9 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[38] at count at StatsGenerator.scala:67) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  YarnScheduler:54 - Adding task set 6.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  TaskSetManager:54 - Starting task 0.0 in stage 6.0 (TID 6, algo-1, executor 1, partition 0, PROCESS_LOCAL, 8333 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:47 INFO  BlockManagerInfo:54 - Added broadcast_9_piece0 in memory on algo-1:38349 (size: 16.0 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  TaskSetManager:54 - Finished task 0.0 in stage 6.0 (TID 6) in 93 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnScheduler:54 - Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - ShuffleMapStage 6 (count at StatsGenerator.scala:67) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - running: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - waiting: Set(ResultStage 7)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - failed: Set()\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - Submitting ResultStage 7 (MapPartitionsRDD[41] at count at StatsGenerator.scala:67), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  MemoryStore:54 - Block broadcast_10 stored as values in memory (estimated size 7.4 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  MemoryStore:54 - Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 1458.0 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on 10.2.216.41:43771 (size: 3.8 KB, free: 1458.5 MB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  SparkContext:54 - Created broadcast 10 from broadcast at DAGScheduler.scala:1039\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[41] at count at StatsGenerator.scala:67) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnScheduler:54 - Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  TaskSetManager:54 - Starting task 0.0 in stage 7.0 (TID 7, algo-1, executor 1, partition 0, NODE_LOCAL, 7765 bytes)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  BlockManagerInfo:54 - Added broadcast_10_piece0 in memory on algo-1:38349 (size: 3.8 KB, free: 5.8 GB)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  MapOutputTrackerMasterEndpoint:54 - Asked to send map output locations for shuffle 2 to 10.2.216.41:37870\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  TaskSetManager:54 - Finished task 0.0 in stage 7.0 (TID 7) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnScheduler:54 - Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - ResultStage 7 (count at StatsGenerator.scala:67) finished in 0.065 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  DAGScheduler:54 - Job 4 finished: count at StatsGenerator.scala:67, took 0.190109 s\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  StatsGenerator:70 - Stats: {\n",
      "  \"version\" : 0.0,\n",
      "  \"dataset\" : {\n",
      "    \"item_count\" : 399\n",
      "  },\n",
      "  \"features\" : [ {\n",
      "    \"name\" : \"_c0\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 3.0,\n",
      "      \"distribution\" : {\n",
      "        \"categorical\" : {\n",
      "          \"buckets\" : [ {\n",
      "            \"value\" : \"0\",\n",
      "            \"count\" : 256\n",
      "          }, {\n",
      "            \"value\" : \"truth\",\n",
      "            \"count\" : 1\n",
      "          }, {\n",
      "            \"value\" : \"1\",\n",
      "            \"count\" : 142\n",
      "          } ]\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c1\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 347.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c2\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 371.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c3\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 355.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c4\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 379.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c5\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 375.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c6\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 363.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c7\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 363.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c8\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 413.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c9\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 382.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c10\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 363.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c11\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 373.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c12\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 373.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c13\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 394.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c14\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 330.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c15\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 379.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c16\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 405.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c17\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 397.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c18\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 357.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c19\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 359.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c20\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 324.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c21\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 361.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c22\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 335.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c23\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 371.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c24\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 365.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c25\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 333.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c26\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 390.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c27\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 384.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c28\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 361.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c29\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 351.0\n",
      "    }\n",
      "  }, {\n",
      "    \"name\" : \"_c30\",\n",
      "    \"inferred_type\" : \"String\",\n",
      "    \"string_statistics\" : {\n",
      "      \"common\" : {\n",
      "        \"num_present\" : 399,\n",
      "        \"num_missing\" : 0\n",
      "      },\n",
      "      \"distinct_count\" : 375.0\n",
      "    }\n",
      "  } ]\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  FileUtil:29 - Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnClientSchedulerBackend:54 - Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnClientSchedulerBackend:54 - Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  YarnClientSchedulerBackend:54 - Stopped\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  MemoryStore:54 - MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  BlockManager:54 - BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  BlockManagerMaster:54 - BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  SparkContext:54 - Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  Main:62 - Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  Main:138 - Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  ShutdownHookManager:54 - Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-a28f1950-201e-4164-99d3-a146f86f382a\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48 INFO  ShutdownHookManager:54 - Deleting directory /tmp/spark-b4805027-1082-47f1-9abf-a8003d3f56a9\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48,705 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2020-10-14 15:51:48,705 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<sagemaker.processing.ProcessingJob at 0x7f6d7d5e0ed0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset=s3_training_data_path,\n",
    "    \n",
    "    # change header to false if not included\n",
    "    dataset_format=DatasetFormat.csv(header=False),\n",
    "    output_s3_uri=s3_baseline_results,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you like, you can download the results from S3 and analyze. In the interest of time, we'll move on to setting up the monitoring schedule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Monitoring Schedule with name: bi-hourly\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'bi-hourly'\n",
    "s3_report_path = 's3://{}/{}/model-monitor/monitoring-job-results'.format(bucket, prefix)\n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=endpoint_name,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.daily(),\n",
    "    enable_cloudwatch_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Tune your model and re-deploy onto the SageMaker Endpoint\n",
    "\n",
    "Alright, we made it pretty far already! Now that we have monitoring enabled on this endpoint, let's imagine that something goes awry. We realize that we need a new model hosted on this RESTful API. How are we going to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's go about getting a new model. Given that the dataset here is pretty small, less than even 500 rows on the training set, why not try out AutoGluon? AutoGluon is a competitive choice here because it will actually augment our data for us. Said another way, Autogluon will make our original dataset larger by using Transformers and masking columns. Pretty cool!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/requirements.txt\n",
    "\n",
    "autogluon\n",
    "sagemaker\n",
    "awscli \n",
    "boto3\n",
    "PrettyTable\n",
    "bokeh\n",
    "numpy==1.16.1\n",
    "matplotlib\n",
    "sagemaker-experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.py\n",
    "\n",
    "import ast\n",
    "import argparse\n",
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import subprocess\n",
    "import sys\n",
    "import boto3\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from timeit import default_timer as timer\n",
    "import time\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "sys.path.insert(0, 'package')\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "    from prettytable import PrettyTable\n",
    "    import autogluon as ag\n",
    "    from autogluon import TabularPrediction as task\n",
    "    from autogluon.task.tabular_prediction import TabularDataset\n",
    "    \n",
    "# ------------------------------------------------------------ #\n",
    "# Training methods                                             #\n",
    "# ------------------------------------------------------------ #\n",
    "\n",
    "def du(path):\n",
    "    \"\"\"disk usage in human readable format (e.g. '2,1GB')\"\"\"\n",
    "    return subprocess.check_output(['du','-sh', path]).split()[0].decode('utf-8')\n",
    "\n",
    "def __load_input_data(path: str) -> TabularDataset:\n",
    "    \"\"\"\n",
    "    Load training data as dataframe\n",
    "    :param path:\n",
    "    :return: DataFrame\n",
    "    \"\"\"\n",
    "    input_data_files = os.listdir(path)\n",
    "    try:\n",
    "        input_dfs = [pd.read_csv(f'{path}/{data_file}') for data_file in input_data_files]\n",
    "        return task.Dataset(df=pd.concat(input_dfs))\n",
    "    except:\n",
    "        print(f'No csv data in {path}!')\n",
    "        return None\n",
    "\n",
    "def train(args):\n",
    "        \n",
    "    is_distributed = len(args.hosts) > 1\n",
    "    host_rank = args.hosts.index(args.current_host)    \n",
    "    dist_ip_addrs = args.hosts\n",
    "    dist_ip_addrs.pop(host_rank)\n",
    "    ngpus_per_trial = 1 if args.num_gpus > 0 else 0\n",
    "\n",
    "    # load training and validation data\n",
    "    print(f'Train files: {os.listdir(args.train)}')\n",
    "    train_data = __load_input_data(args.train)\n",
    "    print(f'Label counts: {dict(Counter(train_data[args.label]))}')\n",
    "    \n",
    "    predictor = task.fit(\n",
    "        train_data=train_data,\n",
    "        label=args.label,            \n",
    "        output_directory=args.model_dir,\n",
    "        problem_type=args.problem_type,\n",
    "        eval_metric=args.eval_metric,\n",
    "        stopping_metric=args.stopping_metric,\n",
    "        auto_stack=args.auto_stack, # default: False\n",
    "        hyperparameter_tune=args.hyperparameter_tune, # default: False\n",
    "        feature_prune=args.feature_prune, # default: False\n",
    "        holdout_frac=args.holdout_frac, # default: None\n",
    "        num_bagging_folds=args.num_bagging_folds, # default: 0\n",
    "        num_bagging_sets=args.num_bagging_sets, # default: None\n",
    "        stack_ensemble_levels=args.stack_ensemble_levels, # default: 0\n",
    "        cache_data=args.cache_data,\n",
    "        time_limits=args.time_limits,\n",
    "        num_trials=args.num_trials, # default: None\n",
    "        search_strategy=args.search_strategy, # default: 'random'\n",
    "        search_options=args.search_options,\n",
    "        visualizer=args.visualizer,\n",
    "        verbosity=args.verbosity\n",
    "    )\n",
    "    \n",
    "    # Results summary\n",
    "    predictor.fit_summary(verbosity=1)\n",
    "\n",
    "    # Leaderboard on optional test data\n",
    "    if args.test:\n",
    "        print(f'Test files: {os.listdir(args.test)}')\n",
    "        test_data = __load_input_data(args.test)    \n",
    "        print('Running model on test data and getting Leaderboard...')\n",
    "        leaderboard = predictor.leaderboard(dataset=test_data, silent=True)\n",
    "        def format_for_print(df):\n",
    "            table = PrettyTable(list(df.columns))\n",
    "            for row in df.itertuples():\n",
    "                table.add_row(row[1:])\n",
    "            return str(table)\n",
    "        print(format_for_print(leaderboard), end='\\n\\n')\n",
    "\n",
    "    # Files summary\n",
    "    print(f'Model export summary:')\n",
    "    print(f\"/opt/ml/model/: {os.listdir('/opt/ml/model/')}\")\n",
    "    models_contents = os.listdir('/opt/ml/model/models')\n",
    "    print(f\"/opt/ml/model/models: {models_contents}\")\n",
    "    print(f\"/opt/ml/model directory size: {du('/opt/ml/model/')}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------ #\n",
    "# Training execution                                           #\n",
    "# ------------------------------------------------------------ #\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('yes', 'true', 't', '1')\n",
    "\n",
    "def parse_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "             formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.register('type','bool',str2bool) # add type keyword to registries\n",
    "\n",
    "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))    \n",
    "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\n",
    "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR']) # /opt/ml/model\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n",
    "    parser.add_argument('--test', type=str, default='') # /opt/ml/input/data/test\n",
    "    parser.add_argument('--label', type=str, default='truth',\n",
    "                        help=\"Name of the column that contains the target variable to predict.\")\n",
    "    \n",
    "    parser.add_argument('--problem_type', type=str, default=None,\n",
    "                        help=(\"Type of prediction problem, i.e. is this a binary/multiclass classification or \"\n",
    "                              \"regression problem options: 'binary', 'multiclass', 'regression'). \"\n",
    "                              \"If `problem_type = None`, the prediction problem type is inferred based \"\n",
    "                              \"on the label-values in provided dataset.\"))\n",
    "    parser.add_argument('--eval_metric', type=str, default=None,\n",
    "                        help=(\"Metric by which predictions will be ultimately evaluated on test data.\"\n",
    "                              \"AutoGluon tunes factors such as hyperparameters, early-stopping, ensemble-weights, etc. \"\n",
    "                              \"in order to improve this metric on validation data. \"\n",
    "                              \"If `eval_metric = None`, it is automatically chosen based on `problem_type`. \"\n",
    "                              \"Defaults to 'accuracy' for binary and multiclass classification and \"\n",
    "                              \"'root_mean_squared_error' for regression. \"\n",
    "                              \"Otherwise, options for classification: [ \"\n",
    "                              \"    'accuracy', 'balanced_accuracy', 'f1', 'f1_macro', 'f1_micro', 'f1_weighted', \"\n",
    "                              \"    'roc_auc', 'average_precision', 'precision', 'precision_macro', 'precision_micro', 'precision_weighted', \"\n",
    "                              \"    'recall', 'recall_macro', 'recall_micro', 'recall_weighted', 'log_loss', 'pac_score']. \"\n",
    "                              \"Options for regression: ['root_mean_squared_error', 'mean_squared_error', \"\n",
    "                              \"'mean_absolute_error', 'median_absolute_error', 'r2']. \"\n",
    "                              \"For more information on these options, see `sklearn.metrics`: \"\n",
    "                              \"https://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics \"\n",
    "                              \"You can also pass your own evaluation function here as long as it follows formatting of the functions \"\n",
    "                              \"defined in `autogluon/utils/tabular/metrics/`. \"))\n",
    "    parser.add_argument('--stopping_metric', type=str, default=None,\n",
    "                        help=(\"Metric which models use to early stop to avoid overfitting. \"\n",
    "                              \"`stopping_metric` is not used by weighted ensembles, instead weighted ensembles maximize `eval_metric`. \"\n",
    "                              \"Defaults to `eval_metric` value except when `eval_metric='roc_auc'`, where it defaults to `log_loss`.\"))      \n",
    "    parser.add_argument('--auto_stack', type='bool', default=False,\n",
    "                        help=(\"Whether to have AutoGluon automatically attempt to select optimal \"\n",
    "                              \"num_bagging_folds and stack_ensemble_levels based on data properties. \"\n",
    "                              \"Note: Overrides num_bagging_folds and stack_ensemble_levels values. \"\n",
    "                              \"Note: This can increase training time by up to 20x, but can produce much better results. \"\n",
    "                              \"Note: This can increase inference time by up to 20x.\"))\n",
    "    parser.add_argument('--hyperparameter_tune', type='bool', default=False,\n",
    "                        help=(\"Whether to tune hyperparameters or just use fixed hyperparameter values \"\n",
    "                              \"for each model. Setting as True will increase `fit()` runtimes.\"))\n",
    "    parser.add_argument('--feature_prune', type='bool', default=False,\n",
    "                        help=\"Whether or not to perform feature selection.\")\n",
    "    parser.add_argument('--holdout_frac', type=float, default=None, \n",
    "                        help=(\"Fraction of train_data to holdout as tuning data for optimizing hyperparameters \"\n",
    "                              \"(ignored unless `tuning_data = None`, ignored if `num_bagging_folds != 0`). \"\n",
    "                              \"Default value is selected based on the number of rows in the training data. \"\n",
    "                              \"Default values range from 0.2 at 2,500 rows to 0.01 at 250,000 rows. \"\n",
    "                              \"Default value is doubled if `hyperparameter_tune = True`, up to a maximum of 0.2. \"\n",
    "                              \"Disabled if `num_bagging_folds >= 2`.\"))    \n",
    "    parser.add_argument('--num_bagging_folds', type=int, default=0, \n",
    "                        help=(\"Number of folds used for bagging of models. When `num_bagging_folds = k`, \"\n",
    "                              \"training time is roughly increased by a factor of `k` (set = 0 to disable bagging). \"\n",
    "                              \"Disabled by default, but we recommend values between 5-10 to maximize predictive performance. \"\n",
    "                              \"Increasing num_bagging_folds will result in models with lower bias but that are more prone to overfitting. \"\n",
    "                              \"Values > 10 may produce diminishing returns, and can even harm overall results due to overfitting. \"\n",
    "                              \"To further improve predictions, avoid increasing num_bagging_folds much beyond 10 \"\n",
    "                              \"and instead increase num_bagging_sets. \"))    \n",
    "    parser.add_argument('--num_bagging_sets', type=int, default=None,\n",
    "                        help=(\"Number of repeats of kfold bagging to perform (values must be >= 1). \"\n",
    "                              \"Total number of models trained during bagging = num_bagging_folds * num_bagging_sets. \"\n",
    "                              \"Defaults to 1 if time_limits is not specified, otherwise 20 \"\n",
    "                              \"(always disabled if num_bagging_folds is not specified). \"\n",
    "                              \"Values greater than 1 will result in superior predictive performance, \"\n",
    "                              \"especially on smaller problems and with stacking enabled. \"\n",
    "                              \"Increasing num_bagged_sets reduces the bagged aggregated variance without \"\n",
    "                              \"increasing the amount each model is overfit.\"))\n",
    "    parser.add_argument('--stack_ensemble_levels', type=int, default=0, \n",
    "                        help=(\"Number of stacking levels to use in stack ensemble. \"\n",
    "                              \"Roughly increases model training time by factor of `stack_ensemble_levels+1` \" \n",
    "                              \"(set = 0 to disable stack ensembling).  \"\n",
    "                              \"Disabled by default, but we recommend values between 1-3 to maximize predictive performance. \"\n",
    "                              \"To prevent overfitting, this argument is ignored unless you have also set `num_bagging_folds >= 2`.\"))\n",
    "    parser.add_argument('--hyperparameters', type=lambda s: ast.literal_eval(s), default=None,\n",
    "                        help=\"Refer to docs: https://autogluon.mxnet.io/api/autogluon.task.html\")\n",
    "    parser.add_argument('--cache_data', type='bool', default=True,\n",
    "                       help=(\"Whether the predictor returned by this `fit()` call should be able to be further trained \"\n",
    "                             \"via another future `fit()` call. \"\n",
    "                             \"When enabled, the training and validation data are saved to disk for future reuse.\"))\n",
    "    parser.add_argument('--time_limits', type=int, default=None, \n",
    "                        help=(\"Approximately how long `fit()` should run for (wallclock time in seconds).\"\n",
    "                              \"If not specified, `fit()` will run until all models have completed training, \"\n",
    "                              \"but will not repeatedly bag models unless `num_bagging_sets` is specified.\"))\n",
    "    parser.add_argument('--num_trials', type=int, default=None, \n",
    "                        help=(\"Maximal number of different hyperparameter settings of each \"\n",
    "                              \"model type to evaluate during HPO. (only matters if \"\n",
    "                              \"hyperparameter_tune = True). If both `time_limits` and \"\n",
    "                              \"`num_trials` are specified, `time_limits` takes precedent.\"))    \n",
    "    parser.add_argument('--search_strategy', type=str, default='random',\n",
    "                        help=(\"Which hyperparameter search algorithm to use. \"\n",
    "                              \"Options include: 'random' (random search), 'skopt' \"\n",
    "                              \"(SKopt Bayesian optimization), 'grid' (grid search), \"\n",
    "                              \"'hyperband' (Hyperband), 'rl' (reinforcement learner)\"))      \n",
    "    parser.add_argument('--search_options', type=lambda s: ast.literal_eval(s), default=None,\n",
    "                        help=\"Auxiliary keyword arguments to pass to the searcher that performs hyperparameter optimization.\")\n",
    "    parser.add_argument('--nthreads_per_trial', type=int, default=None,\n",
    "                        help=\"How many CPUs to use in each training run of an individual model. This is automatically determined by AutoGluon when left as None (based on available compute).\")\n",
    "    parser.add_argument('--ngpus_per_trial', type=int, default=None,\n",
    "                        help=\"How many GPUs to use in each trial (ie. single training run of a model). This is automatically determined by AutoGluon when left as None.\")\n",
    "    parser.add_argument('--dist_ip_addrs', type=list, default=None,\n",
    "                        help=\"List of IP addresses corresponding to remote workers, in order to leverage distributed computation.\") \n",
    "    parser.add_argument('--visualizer', type=str, default='none',\n",
    "                        help=(\"How to visualize the neural network training progress during `fit()`. \"\n",
    "                              \"Options: ['mxboard', 'tensorboard', 'none'].\"))          \n",
    "    parser.add_argument('--verbosity', type=int, default=2, \n",
    "                        help=(\"Verbosity levels range from 0 to 4 and control how much information is printed during fit(). \"\n",
    "                              \"Higher levels correspond to more detailed print statements (you can set verbosity = 0 to suppress warnings). \"\n",
    "                              \"If using logging, you can alternatively control amount of information printed via `logger.setLevel(L)`, \"\n",
    "                              \"where `L` ranges from 0 to 50 (Note: higher values of `L` correspond to fewer print statements, \"\n",
    "                              \"opposite of verbosity levels\"))\n",
    "    parser.add_argument('--debug', type='bool', default=False,\n",
    "                       help=(\"Whether to set logging level to DEBUG\"))  \n",
    "        \n",
    "    parser.add_argument('--feature_importance', type='bool', default=True)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def set_experiment_config(experiment_basename = None):\n",
    "    '''\n",
    "    Optionally takes an base name for the experiment. Has a hard dependency on boto3 installation. \n",
    "    Creates a new experiment using the basename, otherwise simply uses autogluon as basename.\n",
    "    May run into issues on Experiments' requirements for basename config downstream.\n",
    "    '''\n",
    "    now = int(time.time())\n",
    "    \n",
    "    if experiment_basename:\n",
    "        experiment_name = '{}-autogluon-{}'.format(experiment_basename, now)\n",
    "    else:\n",
    "        experiment_name = 'autogluon-{}'.format(now)\n",
    "    \n",
    "    try:\n",
    "        client = boto3.Session().client('sagemaker')\n",
    "    except:\n",
    "        print ('You need to install boto3 to create an experiment. Try pip install --upgrade boto3')\n",
    "        return ''\n",
    "    \n",
    "    try:\n",
    "        Experiment.create(experiment_name=experiment_name, \n",
    "                            description=\"Running AutoGluon Tabular with SageMaker Experiments\", \n",
    "                            sagemaker_boto_client=client)\n",
    "        print ('Created an experiment named {}, you should be able to see this in SageMaker Studio right now.'.format(experiment_name))\n",
    "        \n",
    "    except:\n",
    "        print ('Could not create the experiment. Is your basename properly configured? Also try installing the sagemaker experiments SDK with pip install sagemaker-experiments.')\n",
    "        return ''\n",
    "    \n",
    "    return experiment_name\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = timer()\n",
    "\n",
    "    args = parse_args()\n",
    "        \n",
    "    # Print SageMaker args\n",
    "    print('\\n====== args ======')\n",
    "    for k,v in vars(args).items():\n",
    "        print(f'{k},  type: {type(v)},  value: {v}')\n",
    "    print()\n",
    "    \n",
    "    train()\n",
    "\n",
    "    # Package inference code with model export\n",
    "    subprocess.call('mkdir /opt/ml/model/code'.split())\n",
    "    subprocess.call('cp /opt/ml/code/inference.py /opt/ml/model/code/'.split())\n",
    "    \n",
    "    elapsed_time = round(timer()-start,3)\n",
    "    print(f'Elapsed time: {elapsed_time} seconds')  \n",
    "    print('===== Training Completed =====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "estimator = MXNet(source_dir = 'src',\n",
    "                    entry_point = 'train.py',\n",
    "                    role=role,\n",
    "                    framework_version = '1.7.0',\n",
    "                    py_version = 'py3',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.2xlarge',\n",
    "                    volume_size=100)         \n",
    "\n",
    "s3_path = 's3://sagemaker-us-east-1-181880743555/model-hosting/test_set.csv'\n",
    "\n",
    "estimator.fit(s3_path, wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sagemaker.sklearn.estimator import SKLearn\n",
    "# from sagemaker import get_execution_role\n",
    "\n",
    "# script_path = 'train.py'\n",
    "\n",
    "# # first, let's get the estimator defined \n",
    "# est = SKLearn(entry_point=script_path,\n",
    "#                 instance_type=\"ml.c4.xlarge\",\n",
    "#                 instance_count = 1,\n",
    "#                 role=role,\n",
    "#                 sagemaker_session=sess,\n",
    "#                 py_version = 'py3',\n",
    "#                 framework_version = '0.20.0')\n",
    "\n",
    "# # then, let's set up the tuning framework \n",
    "# from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "# hyperparameter_ranges = {'lr': ContinuousParameter(0.00001, 0.001),\n",
    "#                          'batch-size': IntegerParameter(25, 300)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# objective_metric_name = 'Accuracy'\n",
    "# objective_type = 'Maximize'\n",
    "# metric_definitions = [{'Name': 'Accuracy',\n",
    "#                        'Regex': 'Accuracy: ([0-9\\\\.]+)'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner = HyperparameterTuner(est,\n",
    "#                             objective_metric_name,\n",
    "#                             hyperparameter_ranges,\n",
    "#                             metric_definitions,\n",
    "#                             max_jobs=20,\n",
    "#                             max_parallel_jobs=3,\n",
    "#                             objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg = 'aws s3 cp test_set.csv s3://{}/{}/ && aws s3 cp train_set.csv s3://{}/{}/'.format(bucket, prefix, bucket, prefix)\n",
    "# os.system(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # may complain about not wanting headers \n",
    "# inputs = {'train': 's3://{}/{}/train_set.csv'.format(bucket, prefix),\n",
    "#           'test': 's3://{}/{}/test_set.csv'.format(bucket, prefix)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redeploy to existing SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "job_name = 'sagemaker-scikit-lea-201014-1830'\n",
    "\n",
    "tuner = HyperparameterTuner.attach(job_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-10-14 18:33:33 Starting - Preparing the instances for training\n",
      "2020-10-14 18:33:33 Downloading - Downloading input data\n",
      "2020-10-14 18:33:33 Training - Training image download completed. Training in progress.\n",
      "2020-10-14 18:33:33 Uploading - Uploading generated training model\n",
      "2020-10-14 18:33:33 Completed - Training job completed\n"
     ]
    }
   ],
   "source": [
    "best_estimator = tuner.best_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_estimator.create_model()\n",
    "model_name = model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no autoscaling policy to deregister, continuing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointArn': 'arn:aws:sagemaker:us-east-1:181880743555:endpoint/sagemaker-scikit-learn-2020-10-14-15-12-50-644',\n",
       " 'ResponseMetadata': {'RequestId': '568a69ab-fc94-4812-9b4e-9e8575063b72',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '568a69ab-fc94-4812-9b4e-9e8575063b72',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '114',\n",
       "   'date': 'Wed, 14 Oct 2020 21:01:15 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "def create_model(model, now):\n",
    "    sm_client = boto3.client('sagemaker')\n",
    "    \n",
    "    x = random.randint(1, 100)\n",
    "    \n",
    "    model_name = '{}-{}'.format(model.name, now)\n",
    "    \n",
    "    response = sm_client.create_model(ModelName=model_name,\n",
    "            PrimaryContainer={'ContainerHostname': 'string','Image': model.image_uri, 'ModelDataUrl': model.model_data},\n",
    "            ExecutionRoleArn= 'arn:aws:iam::181880743555:role/service-role/AmazonSageMaker-ExecutionRole-20200929T125134')\n",
    "\n",
    "    return response\n",
    "\n",
    "def get_endpoint_config(model_name, now):\n",
    "    \n",
    "    sm_client = boto3.client('sagemaker')\n",
    "\n",
    "    endpoint_config_name = 'ec-{}-{}'.format(model_name, now)\n",
    "    \n",
    "    response = sm_client.create_endpoint_config(EndpointConfigName= endpoint_config_name,\n",
    "                                            ProductionVariants=[{'VariantName': 'v-{}'.format(model_name),\n",
    "                                                                    'ModelName': model_name,\n",
    "                                                                    'InitialInstanceCount': 1,\n",
    "                                                                    'InstanceType':'ml.m5.large'}])\n",
    "    return endpoint_config_name\n",
    "\n",
    "def update_endpoint(model_name, endpoint_name, now):\n",
    "    \n",
    "    sm_client = boto3.client('sagemaker')\n",
    "\n",
    "    endpoint_config = get_endpoint_config(model_name, now)\n",
    "    \n",
    "    # deregister a scaling policy \n",
    "    resource_id = get_resource_id(endpoint_name)\n",
    "    \n",
    "    client = boto3.client('application-autoscaling')\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        response = client.deregister_scalable_target(ServiceNamespace='sagemaker',\n",
    "                                                    ResourceId=resource_id,\n",
    "                                                    ScalableDimension='sagemaker:variant:DesiredInstanceCount')\n",
    "        \n",
    "    except:\n",
    "        print ('no autoscaling policy to deregister, continuing')\n",
    "    # get monitoring schedules\n",
    "    \n",
    "    try:\n",
    "        response = sm_client.list_monitoring_schedules(EndpointName=endpoint_name,\n",
    "                                                    MaxResults=10,\n",
    "                                                    StatusEquals='Scheduled')\n",
    "        # delete monitoring schedules \n",
    "        for each in response['MonitoringScheduleSummaries']:\n",
    "            name = each['MonitoringScheduleName']\n",
    "            response = sm_client.delete_monitoring_schedule(MonitoringScheduleName=name)\n",
    "            \n",
    "    except:\n",
    "        print ('already deleted the monitoring schedules')\n",
    "    \n",
    "    response = sm_client.update_endpoint(EndpointName=endpoint_name,\n",
    "                                        EndpointConfigName=endpoint_config)\n",
    "    \n",
    "    return response\n",
    "\n",
    "                                \n",
    "now = str(datetime.datetime.now()).split('.')[-1]\n",
    "                                \n",
    "endpoint_name = 'sagemaker-scikit-learn-2020-10-14-15-12-50-644'\n",
    "\n",
    "create_model(model, now)\n",
    "\n",
    "update_endpoint(model_name, endpoint_name, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Automate with Notebook Runner\n",
    "Now we're able to monitor new endpoints, we want the ability to automate this whole flow so that we can do it rapidly. As it so happens, a simple and fast way of doing that is using SageMaker processing jobs, CloudWatch, and Lambda. Luckily we can import all of the infrastructure we need using a simple toolkit, which we'll step through here.\n",
    "\n",
    "GitHub notes are right here: https://github.com/aws-samples/sagemaker-run-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo - make sure they have the right execution role here, add cfn all access, then a trust relationship, then inlines to allow create stack, plus codebuild create project nad start build "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !wget https://github.com/aws-samples/sagemaker-run-notebook/releases/download/v0.15.0/sagemaker_run_notebook-0.15.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !pip install sagemaker_run_notebook-0.15.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !run-notebook create-infrastructure --update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile requirements.txt\n",
    "awscli\n",
    "boto3\n",
    "sagemaker\n",
    "pandas\n",
    "sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# !run-notebook create-container --requirements requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/aws-samples/sagemaker-run-notebook/releases/download/v0.15.0/install-run-notebook.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, __you need to open a system terminal on Studio, cd into the directory where we just downloaded `install-run-notebook.sh`, and run the command `bash install-run-notebook.sh`.__ This will run for a few minutes, then prompt you to refresh your web browser. Do that, and you'll see a new Jupyter Widget!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After restarting your Studio page, click on the spaceship widget on the top lefthand side of your Stuio domain view. Make sure you're actually looking at an ipython notebook while you do this.\n",
    "\n",
    "Using the widget is super simple. Paste in your execution role, which you can find by running `sagemaker.get_execution_role()` locally. Then paste in your ECR image repository, which you can find by opening up the ECR page in the AWS console. It should default to `notebook-runner`, so you can just paste that in directly.\n",
    "\n",
    "Then click the big blue `run now` button, and __this entire notebook is going to run on a SageMaker processing job.__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you do that, you'll want to comment-out those last few cells you ran to install this toolkit and get the infrastructure up and running. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want, you can parameterize this entire notebook using Papermill. Read more about how to do that with the following resources:\n",
    "- Blog post: https://aws.amazon.com/blogs/machine-learning/scheduling-jupyter-notebooks-on-sagemaker-ephemeral-instances/\n",
    "- GitHub repository: https://github.com/aws-samples/sagemaker-run-notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
